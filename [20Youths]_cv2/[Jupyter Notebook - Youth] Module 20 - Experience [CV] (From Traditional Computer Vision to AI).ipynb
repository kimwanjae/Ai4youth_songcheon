{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Traditional Computer Vision to Artificial Intelligence\n",
    "*Version 1.1*\n",
    "\n",
    "위아래로 이동하려면 키보드의 위아래 화살표 키를 사용할 수 있습니다. <br />\n",
    "이 통합 문서에서 코드를 실행하려면 코드 블록을 선택하고 ** Shift + Enter를 누릅니다. ** <br />\n",
    "코드 블록을 편집하려면 Enter 키를 누릅니다.\n",
    "\n",
    "이 통합 문서의 코드는 누적됩니다. (노트북이 닫힐 때까지 정의 된 변수는 계속 사용할 수 있습니다.) <br />\n",
    "따라서 예상치 못한 결과를 피하기 위해 정상에서 시작하여 아래로 작업하십시오!\n",
    "\n",
    "Jupyter Notebook 사용에 대한 추가 도움말을 보려면 위의 메뉴에서 도움말> 사용자 인터페이스 둘러보기를 클릭하십시오. <br />\n",
    "또는 https://jupyter-notebook.readthedocs.io/en/stable/ui_components.html을 방문하십시오.\n",
    "\n",
    "아이디어를 실험하고 테스트 해보십시오. 가장 빠른 학습 방법 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 분류 문제를 해결하기 위해 수동으로 규칙을 정의 할 필요가 없다면 어떻게 될까요?\n",
    "\n",
    "마지막 워크샵에서 다양한 기본 이미지 처리 기술을 실험하고 컴퓨터가 어떻게 보일 수 있는지 탐구했습니다. 그런 다음 카메라 앞에 붙잡은 물건을 사용하여 시스템이 사용자를 인식하도록했습니다. 당신은 다양한 방법을 탐구했고, 창의적 방법의 많은 부분은 규칙 또는 \"if-else\"논리를 정의하는 것과 관련이 있습니다. 예를 들어, \"승인 된\"색상, 위치 또는 조건 조합으로 간주 된 것에 대한 규칙입니다.\n",
    "\n",
    "그러나 이러한 규칙을 수동으로 정의 할 필요가 없다면 어떻게 될까요?\n",
    "\n",
    "** 기계 학습 **은 훈련 데이터를 기반으로 기계를 학습하는 능력에 중점을 둔 인공 지능의 하위 집합입니다. 컴퓨터 비전 분야에 적용되면 정확한 색상 코드에 대한 규칙을 정의하는 대신 \"승인 된\"이미지 또는 \"승인되지 않은\"이미지가 무엇인지 배울 수 있다면 어떨까요?\n",
    "\n",
    "오늘의 워크샵에서는 다양한 컴퓨터 비전 기술을 기계 학습과 결합하여 다양한 문제를 해결할 수있는 방법을 모색합니다.\n",
    "1. 먼저 기계 학습을 설명 할 수있는 간단한 모델을 작성합니다.\n",
    "그런 다음 분류 모델을 작성하는 단계를 살펴 보겠습니다.\n",
    "1. 다음으로 추론을하고 정확성을 탐색하기 위해 분류 모델을 사용합니다.\n",
    "1. 길을 따라, 사용 된 여러 방법과 기술에 대한 한계와 동기를주의 깊게 살펴보십시오.\n",
    "\n",
    "\n",
    "## 카드를 3 개의 가능한 범주 중 1 개로 분류\n",
    "\n",
    "\"액세스 카드\"문제를 다시 한 번 살펴 보겠습니다. <br />\n",
    "아래에는 3 개의 카드 (빨강, 녹색 및 검은 색 카드)와 카메라 앞면에 카드를 놓지 않은 배경 장면이 있습니다.\n",
    "맨 윗줄은 카드가 멀리 떨어져있는 것을 보여 주며 맨 아래 줄은 웹 카메라에 아주 가까이있는 카드를 보여줍니다.\n",
    "\n",
    "\n",
    "<img src=\"images/cards.png\" style=\"float:left;\"/>\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카드가 유효성 검사를 위해 웹 카메라에 가까이 있어야 한다고 가정하면 문제를 파악하고 각 카드 (이미지)의 색상을 비교하여 3 장의 카드 중 어느 카드인지 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Feature Extraction (기능 추출) - 우리가 유추하는 데 사용할 기능을 선택합니다.\n",
    "\n",
    "이 실험에서는 카드를 구별하는 데 도움이되는 색상을 사용하기로 결정했습니다. 그러나 우리는 우리의 색채 피처를 어떻게 선택할 것인가? 특정 지점 (예 : 이미지 중심) 또는 이미지의 평균 색상을 선택해야합니까? 우리는 BGR 이미지의 특정 채널을 사용해야합니까, 아니면 그레이 스케일이나 다른 색상 공간으로 변환해야합니까?\n",
    "\n",
    "우리의 기능을 선택하면 솔루션의 견고성에 영향을 미치며, 관련성이없는 \"기능\"을 선택하는 것은 유용하지 않습니다.\n",
    "\n",
    "예를 들어 카메라 이미지 크기를 사용하여 권한이 부여 된 카드인지 여부를 확인하려고하면 카메라 앞에 어떤 카드가 놓여 있는지에 관계없이 카메라 이미지 크기가 변경되지 않으므로 관련이 없습니다.\n",
    "\n",
    "다른 기능을 시도해 볼 수 있습니다. <br />\n",
    "그러나 그 동안 평균 색상을 피쳐로 사용하여 간단한 실험을 해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's read the images into memory\n",
    "red_card = cv2.imread(\"images/cardred_close.png\")\n",
    "green_card = cv2.imread(\"images/cardgreen_close.png\")\n",
    "black_card = cv2.imread(\"images/cardblack_close.png\")\n",
    "background = cv2.imread(\"images/cardnone.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전처리 및 특징 추출(Preprocessing and Feature Extraction)**\n",
    "\n",
    "때로는 모델에서 허용하는 일관된 형식인지 확인하기 위해 입력 데이터에 대한 사전 처리가 필요할 수 있습니다.\n",
    "\n",
    "데이터를 사전 처리 할 수있는 방법은 무엇입니까?\n",
    "\n",
    "1. 표준 크기로 크기 조정.\n",
    "2. 이미지 방향 변경.\n",
    "3. 특정 색상 공간으로 변환.\n",
    "\n",
    "이 특정 예에서 로드 된 입력 이미지는 기본 BGR 색상 공간에서 이미 일관된 가로 (640x480) 형식으로되어 있습니다. 그러나 우리의 단순한 모델은 이미지의 모든 픽셀을 예측 기능으로 사용하지 않을 것입니다. 대신 모델의 평균 색상을 사용하여 클래스가 속한 클래스를 추론합니다. 그러므로 다음으로 각 이미지에서 평균 색상을 추출하는 방법을 정의 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract our feature (average color)  특징 (평균 색상)을 추출하는 함수를 정의하십시오.\n",
    "def averagecolor(image):\n",
    "    return np.mean(image, axis=(0, 1))   #배열 요소의 평균을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 색상은 3 개의 채널을 가지고 있으므로 (하나의 숫자가 아닌) np.mean을 사용했습니다. np.mean의 작동 방식을 이해하려면 다음 위치의 설명서를 참조하십시오.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html#numpy.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore: 적색 카드와 녹색 카드의 추출 된 특징 (평균 색상)은 무엇입니까? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27.35627604   4.48305664 154.21746094]\n"
     ]
    }
   ],
   "source": [
    "print (averagecolor(red_card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.53976563 133.40338216  61.1089388 ]\n"
     ]
    }
   ],
   "source": [
    "print (averagecolor(green_card))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value generated are different? In fact, their values are very far from each other. This is good! This mean that average color is a good feature for this simple problem.\n",
    "\n",
    "**Now what if we had chosen to use image size as our feature?**\n",
    "\n",
    "생성 된 값이 다른 것에 주목하십시오. 사실, 그 값은 서로 아주 멀리 떨어져 있습니다. 이것은 좋다! 이것은 평균 색이 이 간단한 문제에 대한 좋은 특징이라는 것을 의미합니다.\n",
    "\n",
    "** 이제 이미지 크기를 기능으로 사용하도록 선택한 경우 어떻게 될까요? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "print (red_card.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "print (green_card.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신의 모양 만 알면 레드 카드와 그린 카드를 구분할 수 있습니까?\n",
    "아니! 그들의 모양이 동일하기 때문에.\n",
    "\n",
    "여러분이 그들의 평균 색을 안다면 어떻습니까?\n",
    "\n",
    "위에서 볼 수 있듯이 레드 카드와 그린 카드의 평균 색상은 아주 다릅니다. 그러나 두 카드의 이미지 크기는 정확히 동일합니다! 카드를 구분하기 위해이 기능을 사용하기 때문에 평균 색상을 사용하여 카드 유형을 추론 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 각 이미지 파일의 평균 색상 값과 레이블을 입력하는 변수를 생성합니다. 우리는 모델 훈련을 위해 이것을 나중에 사용할 것입니다. 이 훈련이 어떻게 이루어 졌는지 기억하십니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('red', array([ 27.35627604,   4.48305664, 154.21746094]))\n",
      "('green', array([119.53976563, 133.40338216,  61.1089388 ]))\n",
      "('black', array([70.36474609, 61.85563477, 67.1775651 ]))\n",
      "('none', array([247.9326888 , 241.13666016, 241.89832357]))\n"
     ]
    }
   ],
   "source": [
    "# Store the features (average color) and corresponding label (red/green/black/none) for classification\n",
    "# 분류를 위해 피처 (평균 색)와 해당 레이블 (빨강 / 녹색 / 검정 / 없음)을 저장하십시오.\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "# loop through the cards and print the average color\n",
    "# 카드를 통해 반복하여 평균 색을 인쇄하십시오.\n",
    "for (card,label) in zip((red_card,green_card,black_card,background),(\"red\",\"green\",\"black\",\"none\")):\n",
    "    # zip 함수는 동일한 갯수의 요소값을 갖는 시퀀스 자료형을 묶어주는 역할을 한다.\n",
    "    print((label, averagecolor(card)))\n",
    "    trainX.append(averagecolor(card))\n",
    "    trainY.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 워크샵에서 배열 표현이 [Blue, Green, Red] 순서로 기본 설정되는 방식을 회상 해보세요.\n",
    "\n",
    "레드 카드의 붉은 색 값이 나머지보다 훨씬 더 큽니다. 영주권 카드는 녹색뿐 아니라 청색과 녹색의 값이 더 큽니다.\n",
    "\n",
    "trainX는 이제 특징 벡터 (특징)를 저장하고 trainY는 해당 레이블을 저장합니다.\n",
    "\n",
    "trainX에 저장된 내용과 trainY에 저장된 내용이 궁금한 경우 배열을 인쇄하고 직접 확인하십시오 (위의 인쇄물과 비교).이 시점에서 데이터가 저장되는 방식을 이해하는 것이 도움이됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 27.35627604,   4.48305664, 154.21746094]), array([119.53976563, 133.40338216,  61.1089388 ]), array([70.36474609, 61.85563477, 67.1775651 ]), array([247.9326888 , 241.13666016, 241.89832357])]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX)\n",
    "print(np.array(trainX).shape)      #Note how the 3 channels are stored in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'green', 'black', 'none']\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(trainY)\n",
    "print(np.array(trainY).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 많은 이미지를 찍으면 평균 색상이 항상 정확한 값은 아니며 조명 조건 및 카메라 설정으로 인해 변동될 수 있습니다. 따라서 모델을 학습하는 것은 보통 몇 개의 이미지 이상을 포함합니다. 그러나 개념을 설명하기 위해이 이미지 만 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 K-Nearest Neighbor (kNN) 알고리즘 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카메라 앞에 새 카드를 꽂을 때 가장 비슷한 카드를 결정해야합니다. 정확한 색상 코드를 정의하는 대신 \"**알려진 기존 카드 중 가장 유사한 카드 **\" 라는 각도에서 접근 할 수 있습니다.\n",
    "\n",
    "k-Nearest Neighbors의 개념은 k 개의 가장 유사한 이미지에 대해 레이블이 지정된 이미지 집합을 새 이미지로 검색하는 것입니다. 비슷한 이미지의 라벨을 기반으로 새 이미지의 라벨을 예측합니다.\n",
    "\n",
    "아래에서 k = 1에 대한 실험을 수행 할 것입니다. 즉, 새 이미지와 가장 비슷한 평균 색상을 가진 이미지 1 개를 찾습니다. 그리고 그 이미지의 레이블을 사용하여 새로운 이미지의 레이블을 예측하십시오.\n",
    "\n",
    "이것이 어떻게 이루어 졌는지 분석해 봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 먼저 새로운 이미지를 메모리로 읽어들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_card = cv2.imread(\"images/test/16.png\")\n",
    "new_card_features = averagecolor(new_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 이미지의 특징 (평균 색)과 우리가 알고있는 이미지의 특징 사이의 거리를 계산하십시오\n",
    "linealg.norm [여기] (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html)에 대해 읽어보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
     ]
    }
   ],
   "source": [
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "    \n",
    "print (calculated_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리고 가장 비슷한 카드의 결과는 다음과 같습니다.\n",
    "위의 calculated_distances를 살펴 봄으로써 추측 할 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n"
     ]
    }
   ],
   "source": [
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images / test 하위 폴더를 열고 각 이미지의 실제 색상을 확인하십시오.\n",
    "\n",
    "우리가 사용한 거리 측정 값은 \"np.linalg.norm ()\"입니다. 자세한 내용은 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html 또는 인터넷에서 \"유클리드 거리\"를 참조하십시오. 간단히 말해서, (new_card_features)와 (card)의 배열 값이 얼마나 유사한지를 측정 할 수 있습니다.\n",
    "\n",
    "마지막 줄의 내용을 이해하는 데 시간이 좀 걸립니다. 섹션 1.1의 trainY에 저장된 내용을 상기하십시오.\n",
    "\n",
    "calculated_distances에 저장된 내용을 확인하십시오.\n",
    "np.argmin은 무엇을하나요?\n",
    "\n",
    "힌트 : 필요한 경우 numpy.argmin에 대한 설명서를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
     ]
    }
   ],
   "source": [
    "print(calculated_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.argmin(calculated_distances))   #최소값, 최대값, 혹은 조건에 해당하는 색인(index) 값을 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'green', 'black', 'none']\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n"
     ]
    }
   ],
   "source": [
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 카드를 시험해 봅시다.\n",
    "모델이 실제로 우리가 원하는 것을 예측할 수 있는지 확인하기 위해 폴더를 확인하십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "# First we read the new image into memory\n",
    "new_card = cv2.imread(\"images/test/36.png\")\n",
    "new_card_features = averagecolor(new_card)\n",
    "\n",
    "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "\n",
    "# And here is the result of the which card it is most similar to:\n",
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 카드는 어때?\n",
    "모델이 실제로 우리가 원하는 것을 예측할 수 있는지 확인하기 위해 폴더를 확인하십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n"
     ]
    }
   ],
   "source": [
    "# First we read the new image into memory\n",
    "new_card = cv2.imread(\"images/test/56.png\")\n",
    "new_card_features = averagecolor(new_card)\n",
    "\n",
    "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "\n",
    "# And here is the result of the which card it is most similar to:\n",
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 테스트 카드를 분류 해 봅시다.\n",
    "\n",
    "나쁘지 않다! 우리의 단순한 모델이 지금까지 카드를 정확하게 분류 한 것 같습니다.\n",
    "\n",
    "테스트 하위 폴더의 모든 카드를 루핑하고 분류 해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# 테스트 이미지의 진실을 진술하십시오. 이미지를 보려면 컴퓨터에서 폴더를 엽니 다.\n",
    "realtestY = np.array([\"black\",\"black\",\"black\",\"black\",\"black\",\n",
    "                     \"red\",\"red\",\"red\",\"red\",\"red\",\n",
    "                     \"green\",\"green\",\"green\",\"green\",\"green\",\n",
    "                     \"none\",\"none\",\"none\",\"none\",\"none\"])\n",
    "def evaluateaccuracy(filenames,predictedY):\n",
    "    predictedY = np.array(predictedY)\n",
    "    if (np.sum(realtestY!=predictedY)>0):\n",
    "        print (\"Wrong Predictions: (filename, labelled, predicted) \")\n",
    "        print (np.dstack([filenames,realtestY,predictedY]).squeeze()[(realtestY!=predictedY)])\n",
    "    # 총 예측의 백분율로 일치하는 (올바른) 예측을 계산하십시오.\n",
    "    return \"Correct :\"+ str(np.sum(realtestY==predictedY)) + \". Wrong: \"+str(np.sum(realtestY!=predictedY)) + \". Correctly Classified: \" + str(np.sum(realtestY==predictedY)*100/len(predictedY))+\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드 블록에 대한 결과가 없다는 사실에 놀랐습니까? 이는 정확도 평가를 수행하는 기능만 정의했기 때문입니다. Python의 기능에 대한 자세한 내용을 보려면 [이 링크] (https://www.datacamp.com/community/tutorials/functions-python-tutorial)를 방문하십시오.\n",
    "\n",
    "출력을 보려면 아래의 코드 블록을 실행 해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: none\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      0.80      0.89         5\n",
      "        none       0.83      1.00      0.91         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       0.95      0.95      0.95        20\n",
      "   macro avg       0.96      0.95      0.95        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n",
      "\n",
      "Wrong Predictions: (filename, labelled, predicted) \n",
      "[['58.png' 'green' 'none']]\n",
      "Correct :19. Wrong: 1. Correctly Classified: 95.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"images/test/\"\n",
    "predictedY = []\n",
    "filenames = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    calculated_distances = []\n",
    "    for card in (trainX):\n",
    "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
    "    prediction = trainY[np.argmin(calculated_distances)]\n",
    "    \n",
    "    print (filename + \": \" + prediction) #Print out the inferences\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy (our own custom method to output the filenames of the misclassified entries)\n",
    "print ()\n",
    "print (evaluateaccuracy(filenames,predictedY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 정확도와 리콜은 무엇을 의미합니까? **\n",
    "습득 단계에서 우리가 이걸 겪었다는 것을 기억합니까?\n",
    "\n",
    "진정한 긍정, 거짓 긍정, 참된 네거티브, 거짓 네거티브의 개념을 기억하십시오.\n",
    "\n",
    "예를 들어, 빨강 클래스를 평가하는 경우 :\n",
    "- 빨간색 이미지를 빨간색으로 올바르게 분류하면 이는 사실(true positive)입니다.\n",
    "- 빨간색 이미지를 검정색으로 잘못 분류하면 가짜(false negative)입니다.\n",
    "- 빨간색 이외의 다른 이미지를 빨간색으로 분류하면 false positive입니다.\n",
    "- 비 적색 이미지를 적색이 아닌 것으로 정확하게 분류하면 true negative입니다.\n",
    "\n",
    "정밀도는 (True Positives + False Positive)로 나눈 True Positive의 수입니다. 즉, 빨간색으로 분류 된 것 중 실제로 얼마나 많은 것이 빨간색인지 분류됩니다.\n",
    "\n",
    "리콜은 True Positive 수를 True Positives + False Negatives로 나눈 것입니다. 즉, 적색 이미지를 모두 얻으려고 시도했을 때 얼마나 많은 빨간색 이미지가 빨간색으로 올바르게 분류 되었습니까?\n",
    "\n",
    "정밀도와 리콜에 대해 자세히 알아 보려면 인터넷을 평소와 같이 검색하거나 https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall을 방문하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**잘못 분류 된 이미지를 조사해 보겠습니다.**\n",
    "\n",
    "해당 폴더를 열고 이미지를 확인하십시오.\n",
    "58.png가 잘못 분류 된 것 같습니다. 왜?\n",
    "\n",
    "58.png\n",
    "\n",
    "<img src=\"images/test/58.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "교육용 이미지의 초기 세트를 상기하십시오. <br/>\n",
    "58.png는 \"녹색\"에 대한 교육 이미지보다 훨씬 밝아 보입니다. 왜 이것이 \"없음\"으로 오인 되었습니까? (네 개의 교육 이미지 중에서 \"가장 밝은\")\n",
    "\n",
    "<img src=\"images/cards.png\" style=\"float:left;\"/>\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인간으로서의 우리에게는 58.png가 녹색으로 분류되어야한다고 말하는 것이 쉽습니다.\n",
    "\n",
    "그러나 시스템을 \"훈련\"하는 데 사용 된 기능은 \"평균 색상\"이었고 우리는 하나의 교육 이미지 만 제공했음을 기억하십시오.\n",
    "\n",
    "58.png의 평균 색상이 교육 이미지 (cardgreen_close.png)보다는 배경의 평균 색상 (background.png)에 더 가깝습니다.\n",
    "\n",
    "이미지의 평균 색상을 각각 계산하고 잘못 분류 된 이유를 밝히기위한 연습으로 남겨 둘 것입니다. 이 노트북에서 나중에 도전 1이 될 것입니다.\n",
    "\n",
    "한편 모델을 개선 할 수있는 방법에 대해 생각해 볼 수 있습니까?\n",
    "\n",
    "### 테스트 이미지의 폴더를여십시오!\n",
    "테스트 이미지의 폴더를 열 수 있습니다. 그들은 다른 조명 조건 아래에있는 것처럼 보입니까? 우리는 지금까지 각 색깔의 카드에 대한 단일 예를 사용하여 우리 시스템을 훈련 시켰습니다. 더 많은 교육 이미지가 도움이 될 것이라고 생각합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 더 많은 samples을 이용한 훈련\n",
    " \n",
    "더 많은 샘플을 가지고 훈련 하는게 어떨까요? <br />\n",
    "trainX와 trainY를 얻으려면 1.1 절에서했던 것을 상기하십시오. 잊어 버린 경우 섹션 1.1을 다시 검토하여 코드를 더 잘 이해하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images for the label: red\n",
      "Loading training images for the label: green\n",
      "Loading training images for the label: black\n",
      "Loading training images for the label: none\n"
     ]
    }
   ],
   "source": [
    "trainX2 = []\n",
    "trainY2 = []\n",
    "import os\n",
    "\n",
    "# Let's loop through the training images in the 4 folders in the image subdirectory\n",
    "path = \"images/\"\n",
    "for label in ('red','green','black','none'):\n",
    "    print (\"Loading training images for the label: \"+label)\n",
    "    \n",
    "    #Load all images inside the subfolder\n",
    "    for filename in os.listdir(path+label+\"/\"): \n",
    "        img = cv2.imread(path+label+\"/\"+filename)\n",
    "        img_features = averagecolor(img)\n",
    "        trainX2.append(img_features)\n",
    "        trainY2.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: 지금 우리 모델을 훈련 시키는데 얼마나 많은 이미지를 사용합니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print (len(trainX2))\n",
    "print (len(trainY2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: 하위 폴더를 확인하십시오!\n",
    "컴퓨터의 이미지 디렉토리에서 빨간색, 녹색, 검은 색 및 없음 하위 폴더를 엽니 다. 각 폴더에서 얼마나 많은 이미지를로드합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 교육 이미지를로드 한 후 테스트를 다시 실행하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: green\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      1.00      1.00         5\n",
      "        none       1.00      1.00      1.00         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"images/test/\"\n",
    "filenames = []\n",
    "predictedY = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    calculated_distances = []\n",
    "    for card in (trainX2):\n",
    "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
    "    prediction =  trainY2[np.argmin(calculated_distances)]\n",
    "    \n",
    "    print (filename + \": \" + prediction)\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy\n",
    "print (evaluateaccuracy(filenames,predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 방금 무엇을 했습니까? **\n",
    "\n",
    "우리는 1.1 절에서 kNN에 대한 모델을 \"훈련\"한 방법을 보았습니다. 그런 다음이 모델을 사용하여 1.2 절에서 새 카드가 속한 클래스를 예측했습니다. 우리는 1.3 절에서 훈련 데이터의 증가가 정확도를 높이는 데 도움이 될 수있는 방법을 모색하여 실제로 \"녹색\"이었을 때 \"58.png\"을 잘못 분류하는 오류를 제거했습니다.\n",
    "\n",
    "가장 가까운 이웃을 기반으로 새로운 이미지의 클래스를 예측하기 위해 k 개의 가장 가까운 이웃을 찾는 kNN 알고리즘의 매우 간단한 예를 사용했습니다. 위의 예에서 k의 값은 1입니다. 따라서 가장 가까운 이웃 (계산 된 거리가 가장 작은 이웃) 만 검색하고 가장 가까운 이웃의 클래스를 기반으로 테스트 이미지의 값을 예측했습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반영하기\n",
    "\n",
    "이 방법은 이전 워크샵에서 사용 된 방법과 어떻게 비교됩니까?\n",
    "\n",
    "더 많은 코드 줄을 필요로 했습니까? 규칙을 정의하거나 기계가 스스로 배울 수있게하는 것이 더 좋습니까? 대부분의 경우 수동으로 규칙을 정의하는 것보다 일련의 교육 이미지를 제공하는 것이 더 쉽습니다. 규칙을 쉽게 정의하고 여전히 강력한 시스템을 찾으면 어떤 기법을 사용 했습니까?\n",
    "\n",
    "어떻게 시스템을 더 개선 할 수 있습니까? 접근 방식의 혼합이 더 나아질까요? 이 기능은 모든 유형의 이미지에서 사용할 수 있습니까? 그 이유는 무엇? WorkSheet에 내용을 적어주십시오.\n",
    "\n",
    "<br />\n",
    "<video controls src=\"images/black_red_green.mp4\" style=\"width:400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 분류 모델 구축을위한 기본 단계\n",
    "\n",
    "1 절에서 우리는 kNN 알고리즘에 기반한 매우 간단한 분류 모델을 구현하는 데 신속하게 뛰어 들었습니다.\n",
    "실제로 컴퓨터 비전 모델의 교육은 Keras, Tensorflow, Caffe 및 MXNet과 같은 프레임 워크 또는 Python을위한 Scikit-Learn와 같은 라이브러리를 사용하여 수행됩니다. 이러한 프레임 워크와 라이브러리에는 다양한 도구가 포함되어 있으며 처음부터 모든 것을 코딩 할 필요없이 더 큰 데이터 세트와 알고리즘으로 작업하기가 더 쉽습니다.\n",
    "\n",
    "훈련에는 몇 시간, 며칠 또는 몇 주가 걸릴 수 있으며 종종 GPU가있는 컴퓨터와 더욱 강력한 컴퓨팅 기능이 필요합니다. kNN을 위해 만든 모델은 컨셉을 설명하기 위해 numpy 배열을 사용하는 단순한 모델이었습니다.\n",
    "\n",
    "이제 분류 모델을 작성하는 데 일반적으로 필요한 단계를 살펴 보겠습니다 (이 중 일부는이 연습에서 이미 수행되었습니다).\n",
    "1. 데이터 수집\n",
    "1. 데이터 준비 (청소, 라벨링 등)\n",
    "1. 데이터를 훈련 세트와 시험 세트로 나누기\n",
    "1. 알고리즘 선택 및 모델 교육\n",
    "1. 성과 평가\n",
    "\n",
    "사용할 알고리즘을 선택하는 것은 5 단계 중 하나였습니다. 기계 학습 알고리즘의 경우 데이터 준비가 매우 중요합니다. 잘못된 정보를 입력하면 모델이 잘못 표시됩니다. 데이터는 대표성이 있어야하며 사용 된 기능은 사용자의 목적과 관련이 있어야합니다. 그렇지 않으면 매우 신뢰할 수없는 결과가 발생할 수 있습니다.\n",
    "\n",
    "마찬가지로 이전 사전 처리 및 모델에 사용하는 기능이 중요합니다. 다른 색상의 꽃을 인식하지만 그레이 스케일 이미지 만 사용하여 모델을 훈련 시키려한다고 상상해보십시오 (중요한 색상 기능 제외). 반대로 광학 문자 인식 (OCR)의 경우 색상이별로 유용하지 않을 수 있으며 모델의 선택된 피쳐에 포함되지 않을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machines은 어떨까요?\n",
    "\n",
    "생각해 보면, k-Nearest-Neighbor 알고리즘은 실제로 많은 것을 배우지 못했고 기본적으로 학습 데이터를 저장하고 새로운 이미지에 대한 추론이 필요할 때마다 조회를 수행했습니다.\n",
    "\n",
    "여러분의 수학 수업에서, 여러분은 **y = mx + c?** 의 방정식을 도출하는 것에 대해 배웠던 것을 기억합니까?\n",
    "\n",
    "다른 클래스를 예측하는 데 사용할 수있는 수식이나 수식을 도출 할 수 있다면 어떨까요?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector 란 무엇입니까?\n",
    "\n",
    "X에서 O를 분류해야한다고 상상해보십시오. 모든 X를 O와 가장 잘 구분하는 한 줄을 그릴 수 있습니까?\n",
    "\n",
    "<img src=\"images/svm1.jpg\" style=\"width: 300px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "아마도 우리는 선 (아래 파란색 선)을 그릴 수 있습니다. 그리고 이것은 Support Vector의 간단한 예입니다. 왼쪽 / 맨 위의 항목은 X로 분류 할 수 있으며, 오른쪽 / 아래쪽의 항목은 모두 O로 분류 할 수 있습니다.\n",
    "\n",
    "<img src=\"images/svm2.jpg\" style=\"width: 300px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SVM 뒤에있는 수학은이 워크숍의 범위를 벗어날 것입니다. 그러나 더 많은 것을 읽으시기 바랍니다. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ (이 링크에서는 하나의 선형 벡터가 두 개의 별개의 클래스를 분리 할 수있는 방법을 다이어그램과 함께 보여줍니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계속해서 Python Scikit-Learn 라이브러리를 사용하여 SVM (Support Vector Machines)이 실제로 어떻게 작동하는지 살펴 보겠습니다. 먼저, 지원 벡터의 \"방정식 유도\"를 수행 한 다음 \"방정식을 사용하여\"예측을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 훈련 시키십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainY2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bad7d466e46d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m  \u001b[1;31m#encode labels into numerical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                        \u001b[1;31m#encode labels into numerical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mencodedtrainY2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#encode labels into numerical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainY2' is not defined"
     ]
    }
   ],
   "source": [
    "# Since SVM uses numerical values, we first encode our labels into numerical\n",
    "# SVM은 숫자 값을 사용하기 때문에 먼저 레이블을 숫자로 인코딩합니다\n",
    "from sklearn.preprocessing import LabelEncoder  #encode labels into numerical\n",
    "encoder = LabelEncoder()                        #encode labels into numerical\n",
    "encodedtrainY2 = encoder.fit_transform(trainY2) #encode labels into numerical\n",
    "\n",
    "from sklearn import svm\n",
    "model = svm.SVC(gamma=\"scale\", decision_function_shape='ovr')\n",
    "model.fit(trainX2, encodedtrainY2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder는 무엇을합니까? 함수 결과를 살펴 보겠습니다.\n",
    "LabelEncoder [here] (https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)에 대한 자세한 내용을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print (encodedtrainY2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM에 대한 심층적인 이해는 이 모듈의 범위를 벗어나지만 여기에서 자세히 배우십시오 [here](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 SVM 모델을 얻었습니다.\n",
    "\n",
    "### 예측을 실행합시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: green\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      1.00      1.00         5\n",
      "        none       1.00      1.00      1.00         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"images/test/\"\n",
    "filenames = []\n",
    "predictedY = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    prediction = model.predict([img_features])[0]\n",
    "    \n",
    "    #decode the prediction\n",
    "    # 디코딩 예측\n",
    "    prediction = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    print (filename + \": \" + prediction)\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "# 정확성 평가 (sklearn 패키지는 유용한 보고서를 제공합니다)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy\n",
    "print (evaluateaccuracy(filenames,predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어느 것이 더 정확합니까?\n",
    "\n",
    "SVM이 kNN보다 더 정확한 분류를 얻는데 더 효과적이라고 생각하십니까?\n",
    "\n",
    "그것은 문제에 달려 있습니다. SVM의 경우이 워크샵의 범위를 벗어나는 튜닝해야 할 다른 매개 변수도 있습니다. 이 매개 변수는 모델 생성 프로세스를 안내합니다. 예를 들어, 모델은 생성 할 지원 벡터의 종류를 알아야합니다. 일부 데이터 세트에서는 \"직선\"이 작동하지만 다른 경우에는 곡선 또는 복잡한 지원 벡터가 필요할 수 있습니다.\n",
    "\n",
    "예를 들어 아래의 Os와 X를 분류하기 위해 직선을 맞추려고합니다. 아마도 원 대신 방정식이 필요할 수도 있습니다.\n",
    "\n",
    "<img src=\"images/svm3.jpg\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "자세한 내용을 보려면이 섹션의 끝에있는 링크를 참조하십시오.\n",
    "\n",
    "지금까지 trainX2와 trainY2를 사용하여 모델을 훈련 한 다음 모델을 별도의 이미지 세트에 대해 테스트 한 결과 성능이 좋은 것으로 보였습니다. 그러나 작은 테스트 세트에서 잘 작동한다고해서 항상 정상적으로 작동한다는 것을 의미하지는 않습니다. 이전에 테스트하지 않은 다른 이미지를 다시 테스트 해 봅시다. 인간의 눈은 어떤 색인지 쉽게 알 수 있습니다. 그러나 지금까지 완벽하게 작동하는 것으로 보이는 모델은 정확하게 분류 할 수 있습니까?\n",
    "\n",
    "<img src=\"images/cardtestagain.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n"
     ]
    }
   ],
   "source": [
    "imagenew = cv2.imread(\"images/cardtestagain.png\")\n",
    "imagenew_features = averagecolor(imagenew)\n",
    "prediction = (model.predict([imagenew_features])[0])\n",
    "\n",
    "#decode the prediction from numerical to labels\n",
    "print(encoder.inverse_transform([prediction])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 무엇이 잘못 되었습니까?\n",
    "\n",
    "불행히도 이미지가 빨간색 대신 녹색으로 잘못 분류 된 것처럼 보입니다. <br />\n",
    "SVM 모델이이 워크샵의 범위를 벗어나는 수학을 깊이 파고 들지 않으면 서이 인스턴스에서 잘못 분류 된 이유를 파헤치는 것은 어려울 것입니다. 간단한 비유는 곡선을 직선의 방정식에 맞추기가 어려울 수 있다는 것입니다. y = mx + c가 커브에 사용하는 잘못된 방정식이되는 것과 같습니다.\n",
    "\n",
    "**Side Tip:** 기계 학습을 사용하여 솔루션을 설계 할 때 가장 정확한 모델을 교육하는 것을 목표로 삼지만 모델이 올바른 결과를 제공하지 못할 수도있는 우발적 인 상황을 계획하는 데 약간의 시간이 필요합니다. 또한 애플리케이션에 잘못된 결과가 미치는 영향을 고려하고 위험을 완화하기위한 조치를 취하십시오. 예를 들어, 컴퓨터 비전에 의해 안내되는 장비의 일부인 경우, 무언가에 충돌하기 전에 비상 정지를 트리거하는 데 사용할 수있는 다른 센서가 있습니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편, 우리의 kNN 알고리즘은 동일한 이미지에 대해 어떻게 보았나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "calculated_distances = []\n",
    "for card in (trainX2):\n",
    "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
    "print(trainY2[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 결과로 kNN이 항상 더 신뢰할 만하다는 것을 의미합니까?\n",
    "\n",
    "하나 더 이미지를 사용해 봅시다 :\n",
    "\n",
    "<img src=\"images/cardtestagain2.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: green\n",
      "kNN: none\n"
     ]
    }
   ],
   "source": [
    "imagenew = cv2.imread(\"images/cardtestagain2.png\")\n",
    "imagenew_features = averagecolor(imagenew)\n",
    "calculated_distances = []\n",
    "for card in (trainX2):\n",
    "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
    "    \n",
    "print(\"SVM: \"+str(encoder.inverse_transform([ model.predict([imagenew_features])[0] ])[0]))\n",
    "print(\"kNN: \"+str(trainY2[np.argmin(calculated_distances)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 이미지에서 왜 kNN이 녹색을 none이라고 알고리즘을 잘못 분류했는지 추측 할 수 있습니까?\n",
    "\n",
    "이유를 알아 내기 위해 이미지의 평균 색상을 계산할 수 있습니다.\n",
    "\n",
    "그리고 여러분은 이러한 문제를 완화하기 위해 더 많은 이미지로 모델을 훈련시킬 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: SVM 뒤에있는 수학은이 워크숍의 범위를 벗어날 것입니다. 그러나 더 많은 것을 읽으시기 바랍니다. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ _ (링크에서 하나의 선형 벡터가 두 개의 별개 클래스를 분리 할 수있는 방법을 다이어그램으로 보여줍니다)\n",
    "\n",
    "그러나이 실험에서는 2 개 이상의 클래스를 구분하는 데 사용했습니다. SVM을 사용하여 다중 클래스 분류에 대해 자세히 알아보고 https://scikit-learn.org/stable/modules/svm.html#multi-class-classification에있는 설명서를 사용하여 코드 샘플을 볼 수 있습니다. 인터넷 검색을 잊지 마십시오. 당신이 더 많은 도움이 필요하면."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! \n",
    "\n",
    "## 이제 손을 댈 때가되었습니다!\n",
    "\n",
    "당신은 기계 학습에 대한 아주 빠른 소개를 마치고 규칙 기반 접근법에서 기계 학습 접근법으로의 진보를 보았습니다. 또한 훈련 데이터를 사용하여 분류를 위해 kNN 및 SVM 모델을 교육하고 몇 가지 제한 사항을 비롯하여 결과를 확인했습니다. 확실히 배울 것이 더 많습니다,하지만 당신은 이미 건물을 시작할 수 있습니다!\n",
    "\n",
    "언제든지 도움이 필요할 때마다 친숙한 인터넷을 검색 할 수 있습니다. 다음은 도움이 될 간단한 링크입니다 : <br />\n",
    "- https://docs.opencv.org/4.0.0/d2/d96/tutorial_py_table_of_contents_imgproc.html\n",
    "- https://scikit-learn.org/stable/documentation.html\n",
    "\n",
    "**Tip :** 이전 워크샵에서 픽업 한 기본적인 컴퓨터 비전 기술을 기억하십시오. 모델에 먹이를 주는 데 유용한 기능이 무엇인지 생각한대로 사용할 수 있습니다. 색 공간, 임계 값, 등고선 탐지, 기하학적 변환, 이미지 수위 배열 직접 조작 등. 때로는 기본 방법이 현재의 작업에 가장 효과적 일 수 있습니다.\n",
    "\n",
    "무엇을 구축 하든지 목적과 목적을 염두에두고 가능한 여러 가지 접근 방식을 생각하십시오. 또한 기계 학습 알고리즘이 잘못된 분류를 내리고 위험을 완화 할 방법을 계획 할 때 발생할 수있는 영향을 염두에 두십시오. 예를 들어, 모델이 적색 카드를 잘 식별하지만 녹색과 파란색 카드를 혼합하는 경우 녹색과 파란색 대신에 적색 카드를 사용하여 솔루션을 설계 할 수 있습니다. 그리고 녹색 / 검은 색 카드가 감지되면 경비 요원에게 두 번째 계층 검사를 수행하도록 요구하는 것과 같은 다른 검사 단계를 추가 할 수 있습니다.\n",
    "\n",
    "오류가 발생할 수있는 다른 시나리오와 마찬가지로 다른 기술이나 하드웨어 센서로 실제 솔루션의 디자인을 보완하는 방법을 탐색하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: \"images / test / 58.png\"의 평균 색상은 무엇입니까?\n",
    "\n",
    "그것이 처음에 잘못 분류 된 것을 기억합니까? 우리가 이유를 생각해 봅시다.\n",
    "결과를 변수 \"cha1\"에 저장하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208.77603841 223.6275293  120.75616536]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: \"images / background.png\"의 평균 색상은 무엇입니까?\n",
    "\n",
    "결과를 변수 \"cha2\"에 저장하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247.9326888  241.13666016 241.89832357]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: \"images / cardgreen_close.png\"의 평균 색상은 무엇입니까?\n",
    "\n",
    "결과를 변수 \"cha3\"에 저장하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.53976563 133.40338216  61.1089388 ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: 배경과 58.png. cha1과 cha2 사이의 거리 계산\n",
    "\n",
    "유클리드 거리가 1.2 절에서 어떻게 계산되었는지 기억해보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121546.20205090738"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: 58.png vs Green. cha1과 cha3 사이의 거리 계산\n",
    "\n",
    "유클리드 거리가 1.2 절에서 어떻게 계산되었는지 기억해보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166010.59195424852"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 4 (58.png vs. 배경)의 거리 또는 Challenge 5 (58.png vs. 녹색)의 거리가 더 작습니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거리가 짧을수록 유사도가 높아집니다. 따라서 58.png는 4 \"훈련 이미지\"를 기반으로 한 녹색 카드보다 배경과 더 비슷하게 분류되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: 기능을 추출하기 전에 이미지를 그레이 스케일로 변환하도록 averagecolor 함수를 수정하십시오.\n",
    "\n",
    "이전 워크샵에서 이미지를 그레이 스케일로 변환하는 방법을 상기 해보십시오. averagecolor 대신 평균 그레이 스케일을 사용할 때 kNN 모델의 정확도를 계산합니다. (예 : 업데이트 된 averagecolor 함수를 사용하여 1.3 절 실행. 훈련 및 추론 단계 모두 다시 실행) 새로운 정확도 란 무엇입니까? 더 잘 수행됩니까? 왜 그렇게 생각하니?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: black\n",
      "37.png: black\n",
      "38.png: black\n",
      "39.png: black\n",
      "40.png: black\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: none\n",
      "59.png: green\n",
      "60.png: black\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       0.45      1.00      0.62         5\n",
      "       green       1.00      0.60      0.75         5\n",
      "        none       0.83      1.00      0.91         5\n",
      "         red       0.00      0.00      0.00         5\n",
      "\n",
      "   micro avg       0.65      0.65      0.65        20\n",
      "   macro avg       0.57      0.65      0.57        20\n",
      "weighted avg       0.57      0.65      0.57        20\n",
      "\n",
      "\n",
      "Wrong Predictions: (filename, labelled, predicted) \n",
      "[['36.png' 'red' 'black']\n",
      " ['37.png' 'red' 'black']\n",
      " ['38.png' 'red' 'black']\n",
      " ['39.png' 'red' 'black']\n",
      " ['40.png' 'red' 'black']\n",
      " ['58.png' 'green' 'none']\n",
      " ['60.png' 'green' 'black']]\n",
      "Correct :13. Wrong: 7. Correctly Classified: 65.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\testai\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 아래 기능을 수정할 수 있습니다.\n",
    "\n",
    "def averagecolor_grey(image):\n",
    "    return np.mean(image, axis=(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Topic: 분류(classification) 이외에, 우리는 배운 것을 사용하여 색깔이있는 카드가 이미지 안에있는 지역을 현지화(localize) 할 수 있습니까?\n",
    "\n",
    "모델의 평균 색을 모델의 특징으로 사용했기 때문에 이미지 전체를 분류하는 대신 이미지 내의 각 픽셀을 분류하는 데 사용할 수도 있습니까?\n",
    "\n",
    "어떤 픽셀이 \"색상 등급\"에 속하는지 알면 이미지에서 카드의 위치를 파악하는 데 도움이 될 수 있습니다. 이것은 _localization_의 예입니다.\n",
    "\n",
    "이 실험을하기 위해 3 개의 샘플 이미지를 사용합시다. images / cardblack.png, images / cardred.png, images / cardgreen.png\n",
    "\n",
    "처음에는 그림에 빨간 카드가 어디에 있는지 확인할 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn 패키지의 kNN 분류기를 사용하는 더 간단한 방법이 있습니다.\n",
    "# 작동 원리를 알았으므로 이제는 처음부터 쓰지 않아도됩니다.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=1)\n",
    "knnmodel.fit(trainX2, trainY2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_localization = cv2.imread(\"images/cardred.png\")\n",
    "\n",
    "# First we will need to arrange the pixels into a format that our model accepts (a 2D array)\n",
    "# We can use the numpy array methods to help us do that easily\n",
    "# 먼저 모델에서 허용하는 형식 (2 차원 배열)으로 픽셀을 정렬해야합니다.\n",
    "# numpy 배열 방법을 사용하여 쉽게 할 수 있습니다.\n",
    "temp = card_localization.reshape((307200,3)) \n",
    "\n",
    "#predict the color of each pixel\n",
    "# 각 픽셀의 색을 예측\n",
    "prediction = knnmodel.predict(temp)             \n",
    "\n",
    "# We can arrange the predictions back into the shape of the image that we are familiar with\n",
    "# 우리는 예측을 우리가 익숙한 이미지의 모양으로 되돌릴 수 있습니다.\n",
    "masklabels = (prediction.reshape((480,640)))\n",
    "\n",
    "#create a mask for the class we are interested in! (Recall what are masks in the previous workshop)\n",
    "# 우리가 관심있는 클래스의 마스크 생성! (이전 워크샵에서 마스크 란 무엇인지 기억하십시오)\n",
    "canvas = np.zeros(card_localization.shape[:2],dtype=\"uint8\")\n",
    "canvas[masklabels==\"red\"]=255\n",
    "\n",
    "#get the largest blob and find the location\n",
    "# 가장 큰 얼룩을 찾아서 위치를 찾는다.\n",
    "(cnts,_) = cv2.findContours(canvas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = sorted(cnts, key=lambda cnts: cv2.boundingRect(cnts)[1])[:1]  #sort contours from top to bottom.\n",
    "for (i, c) in enumerate(cnts):    \n",
    "    (x, y, w, h) = cv2.boundingRect(c)                  \n",
    "    cv2.rectangle(card_localization, (x,y), (x+w,y+h), (0,255,0),2)     # Draw the bounding boxes in red\n",
    "\n",
    "\n",
    "#Display the results\n",
    "cv2.imshow(\"Localization\",canvas)\n",
    "cv2.imshow(\"Marked Card\",card_localization) #see the bounding box drawn here\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 개념을 사용하여 기본적으로 각 픽셀에 클래스 레이블을 할당하는 _image segmentation_을 수행 할 수도 있습니다. 아래 예제에서 \"빨강\", \"녹색\", \"검정\"및 \"없음\"레이블 (흰색으로 표시)을 시각화 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_localization = cv2.imread(\"images/cardred.png\")\n",
    "\n",
    "# First we will need to arrange the pixels into a format that our model accepts (a 2D array)\n",
    "# We can use the numpy array methods to help us do that easily\n",
    "# 먼저 모델에서 허용하는 형식 (2 차원 배열)으로 픽셀을 정렬해야합니다.\n",
    "# numpy 배열 방법을 사용하여 쉽게 할 수 있습니다.\n",
    "temp = card_localization.reshape((307200,3)) \n",
    "\n",
    "#predict the color of each pixel\n",
    "# 각 픽셀의 색상 예측\n",
    "prediction = knnmodel.predict(temp)             \n",
    "\n",
    "# We can arrange the predictions back into the shape of the image that we are familiar with\n",
    "masklabels = (prediction.reshape((480,640)))\n",
    "\n",
    "#create a mask for the class we are interested in! (Recall what are masks in the previous workshop)\n",
    "canvas = np.zeros(card_localization.shape,dtype=\"uint8\")\n",
    "canvas[masklabels==\"green\"]=(0,255,0)\n",
    "canvas[masklabels==\"red\"]=(0,0,255)\n",
    "canvas[masklabels==\"black\"]=(0,0,0)\n",
    "canvas[masklabels==\"none\"]=(255,255,255)\n",
    "\n",
    "#Display the results\n",
    "cv2.imshow(\"Image Segmentation\",canvas)\n",
    "cv2.imshow(\"Original Card\",card_localization)\n",
    "#cv2.imshow(\"Masked Image\",cv2.bitwise_and(card_localization,card_localization,mask=canvas))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 분할은 의료 이미징 및 비디오 감시와 같은 애플리케이션에 사용됩니다. 이것은 이미지에 무엇이 있는지를 알아야 할뿐만 아니라 다른 이미지보다 더 중요한 이미지의 부분을 식별하기 때문입니다. 예를 들어, 의학적 스캔에서 가능한 종양을 표시하거나 CCTV에서 의심스러운 사람을 표시합니다.\n",
    "\n",
    "우리 모델에는 몇 가지 불완전 함이 있고 손가락의 일부분이 어떻게 검은 색이나 빨간색으로 잘못 추측되었는지 알 수 있습니다. 그럼에도 불구하고이 예제는 이미지 세분화의 개념을 설명하는 역할을합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Sklearn kNN 모델은 기본적으로 어떤 거리 계산 메트릭을 사용합니까? 우리가 사용한 유클리드 거리인가?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html 을 조회 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric : 문자열 또는 호출 가능, 기본값 'minkowski'\n",
    "# 트리에 사용할 거리 메트릭. 기본 메트릭은 minkowski이며 p = 2는 표준 유클리드 메트릭과 같습니다.\n",
    "# 사용 가능한 메트릭 목록은 DistanceMetric 클래스의 설명서를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: kNN 알고리즘을 사용하여 \"images / cardmixed.png\"에서 빨간색 카드를 제외하고 빨간색 픽셀의 비율과 해당 이미지의 총 픽셀 수를 계산합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15045247395833333\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 이 경우 모델을 사용하여 색상을 피쳐로 사용했기 때문에 모델을 사용하여 각 픽셀의 클래스를 예측할 수 있었습니다. 그러나 이 방법은 다른 모델에 사용 된 기능에 따라 항상 적용 가능한 것은 아닙니다. 다른 지역화 방법을 탐색하기 위해 인터넷을 검색 할 수 있습니다.\n",
    "\n",
    "Artificial Neural Networks (ANN), 특히 Convolutional Neural Networks (CNN)는 훨씬 더 높은 수준의 효율성 때문에 최근에 매우 인기가 있습니다. 다음 워크샵에서는 사전 훈련 된 CNN의 사용법과 함께 Intel Myriad VPU를 기반으로하는 NCS2 (Intel Movidus Neural Compute Stick 2)를 사용하여 추론 할 수있는 방법을 살펴 봅니다.\n",
    "\n",
    "DJI Tello 무인 비행기에 대해 들어 보셨습니까? 그들은 컴퓨터 비전을 사용하여 비행 품질을 향상시키고 Intel MYRIAD VPU의 도움을받습니다!\n",
    "https://www.movidius.com/news/hello-tello-ryze-announces-intel-myriad-vpu-powered-toy-drone\n",
    "\n",
    "이 수업의 초반에 우리는 다른 센서로 머신 비전을 보완하는 방법에 대해서도 논의했습니다. 다른 센서는 Tello drone에 내장되어 있다고 생각합니까?\n",
    "https://www.ryzerobotics.com/tello/specs\n",
    "\n",
    "범위 파인더와 기압계는 무엇을 감지하는데 사용되는 것이라고 생각하십니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
