{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs Visualisation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous workshops, you have learnt how to train different machine learning models and also use these trained models to make predictions. However, we have not focused on how to evaluate the performance of the model or visualise the outputs from the model. As such, we will now focus on how we can use different visualisation tools to help evaluate the performance of the model. In this notebook, you will learn how to utilise confusion matrix and heatmaps to evaluate your model. These tools will help to identify the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to understand the concept of accuracy. How accurate does a model have to be before it is a good model? The answer depends on the use case of the model. For example, the model has to be very close to 100% accuracy if it involves huge financial amounts or illnesses identification. This is because these are important issues that may affect many people if the model were to be wrong by just fractions of a percent. On the other hand, if you were predicting movie recommendations or video recommendations it may be ok if it were less accurate as the implications of a wrong prediction are not as serious as the previously mentioned scenarios. Furthermore, it is also important to understand that it is almost impossible to obtain 100% accuracy as in order to do so, you will need to be able to collect every single data that is relevant to the problem. As such, you will need to balance the accuracy of the model with the amount of data that you have and also the use case. However, this does not mean that it is ok to stop after obtaining a low accuracy. You still need to try and see if you can make it better. Only when you have tried many ways and are unable to improve the accuracy then can you decide if the accuracy is good enough for the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see how we can use the visualisation tools for the machine learning techniques that you have tried in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the K-Nearest Neighbours algorithm on the Iris Flower dataset. Import the dataset as a dataframe df and label encode the dataset. Label encoding is the conversion of categories into numerical groups. Write the code to label encode the dataset as a function (label_encode) instead. This will allow the label encoding code to be re-used if necessary in the notebook. Read this [article](https://www.codementor.io/kaushikpal/user-defined-functions-in-python-8s7wyc8k2) to find out how to write a user defined function. When we code, it is always useful to write functions if we think that the functions would be used more than once in the script. Expand the code below to import and label encode the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df):\n",
    "    \n",
    "    return df_labelled\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = label_encode(df)\n",
    "y_values = df['class']\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the data into train and test sets and also standardise the data. Keep 30% of the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a K-Nearest Neighbours algorithm with the dataset. Remember to use the optimal number of neighbours (6) found in the earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of the model, we have to first predict the flower type or class based on the test set. Use the .predict on KNN to predict the flower classes based on the test set. Store the predictions as y_predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a tool known as a confusion matrix to help us to evaluate the model outputs. Read this [article](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) to find out more about the confusion matrix. What does a true positive refer to? What does a false negative refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix discussed above is mainly for a 2 class classification problem. In the case of the Iris dataset, it is a multi-class problem as there are more than 2 different classes. In fact, there are 3 different classes or flower types. Thus, the confusion matrix will for a multi-class scenario is an extension of the 2 class confusion matrix. As such, a multi-class confusion matrix may help identify which classes were wrongly classified by the model. Let us try to obtain the confusion matrix. Try the code below to import the confusion matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in the predicted values and the actual y values into the confusion matrix function. Try out the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_predict))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is 100% accurate, there should only be non-zero numbers in the diagonal and zeroes everywhere else in the matrix. This is because the diagonals are where the predictions exactly match the actual data. From the above, we can see that the model is quite accurate and only 1 data point was wrongly classified by the model. Based on the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for the confusion_matrix function, are you able to find out which point was misclassified? What was the data point's original class and what was the data point's predicted class? <font color=blue>Hint: The rows represent the actual classes whereas the columns represent the predicted classes. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also present the confusion matrix in an easy to understand heatmap. We can use the library seaborn to help us. Make sure seaborn is installed in your virtual environment. After doing so, run the code below to import seaborn as sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the code below to see the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create new columns within dataframe\n",
    "df['Actual'] = y_test\n",
    "df['Predicted'] = y_predict\n",
    "\n",
    "# Use pd.crosstab to count the frequency of the classes in actual and predicted\n",
    "freq = pd.crosstab(df.Actual,df.Predicted)\n",
    "\n",
    "# Use sns.heatmap to plot the heatmap\n",
    "sns.heatmap(freq,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the same methods to the decision tree. First, train a decision tree on the same dataset using max_depth of 2 and min_samples_split of 6. Do you remember what max_depth and min_samples_split refer to? You can refer to the previous notebook (Model_Outputs) for more information on max_depth and min_Samples_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the classes using the x_test_scale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any wrong classification of data? If so, how many points were wrongly classified? What were the actual and predicted classses of the wrongly classified points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the heatmap for this confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the confusion matrices, which do you think is a better model for the problem? Why is it a better model? Discuss with your friends and list your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when accuracy is similar, there are also other metrics to also consider. These include precision, recall and F1 score. Please read this [link](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9) and this [link](https://medium.com/@raghaviadoni/evaluation-metrics-i-precision-recall-and-f1-score-3ec25e9fb5d3) for more information on these metrics. How would you use these metrics to help decide on your model choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
